# Clustering and Classification

## Analysis Exercises

```{r}
date()
```

## Exercise 2

```{r}

library(MASS)

# load the data
data("Boston")

# explore the data set
str(Boston)
summary(Boston)

```
Description of the data set:

Boston data set, is a collection of data regarding housing values in suburbs of Boston. It checks 14 items on 506 housings, which means we have 14 variables and 506 observations in the data set. (14 columns and 506 rows)
All variables are numerical including 2 integers and 12 floating-points.

## Exercise 3

```{r}

library(MASS)
library(tidyr)
library(corrplot)

# show summaries of the variables
summary(Boston)

# plot matrix of the variables
pairs(Boston)

# calculate the correlation matrix and round it
cor_matrix <- cor(Boston) 

# print the correlation matrix
cor_matrix

#round the data
round(cor_matrix, digits = 2)

# visualize the correlation matrix
corrplot(cor_matrix, method="circle")


```
First, I showed the summaries of variables, then plotted the matrix of scatterplots by using "pairs" function. From matrix of scatterplots we can see a scatterplot for each pair of variables if we set them in a matrix. Then I showed the correlation between each pair of variØ´bles by using the "corrplot" function. The Corrplot illustrates the positive relationship between pairs of variables with red color and negative relationship with blue color. The darker the colors, the stronger the relationship between the variables. The very dark blue color in the corplot relates to relationship between each variable with itself, which is the srtongest relationship but it is not a meaningful relationship. 

## Exersice 4

```{r}

# standardize variables
boston_scaled <- scale(Boston)
boston_scaled <- as.data.frame(scale(Boston))

# scaled crime rate
boston_scaled$crim <- as.numeric(boston_scaled$crim)

# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

# save the correct classes from test data
correct_classes <- boston_scaled$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)


```
To adjust the data set for further analysis, we need to standardize/scale the whole data set. We can do scaling because all variables in data set are numerical. In the scaling we subtract the column means from the corresponding columns and divide the difference with standard deviation. The scaled data become much smaller and even negative in some cases.

## Exercise 5

```{r}

# linear discriminant analysis
lda.fit <- lda(crime ~ . , data = train)

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(factor(train$crime))

# plot the lda results
plot(lda.fit, col = classes, pch = classes, dimen = 2)

```

## Exercise 6

```{r}

ind <- sample(nrow(boston_scaled),  size = nrow(boston_scaled) * 0.8)
train <- boston_scaled[ind,]
test <- boston_scaled[-ind,]

# save the crime categories from test set
correct_classes <- test$crime

# remove the categorical crime variable from test set
test <- dplyr::select(test, -crime)

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)

```
comment: From table, we see that our model has had the best prediction for "high" category/label. Because all 23 high-labeled in training set, labeled as high in test set. While the most errors has been occurred in prediction of "low" category/label because table shows 13 correct labeled values, and 7 wrong labeled values in test set.

## Exercise 7

```{r}

library(ggplot2)

# reload the data
data("Boston")

# standardize variables
boston_scaled <- scale(Boston)
boston_scaled <- as.data.frame(scale(Boston))

# euclidean distance matrix
dist_eu <- dist(Boston, method = "euclidean")

# k-means clustering
km <- kmeans(Boston, centers = 4)

# plot the Boston dataset with clusters
pairs(Boston, col = km$cluster)

set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

# k-means clustering
km <- kmeans(Boston, centers = 2)

# plot the Boston dataset with clusters
pairs(Boston, col = km$cluster)

```
comment: After calculating the "total within sum of squares", we see two radical drops in qplot, then the optimal number of cluster is 2. Again we ran k-means algorithm with 2 clusters and saw the distribution of data for each pair of variables while divided into two clusters/categories, one cluster in red color and the other one in black color.
